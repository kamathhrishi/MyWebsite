---
layout: post
title:  "Approaching Data Centric LLM Finetuning"
date:   2024-01-06 16:42:52 +0530
categories: jekyll update
---

<center>
<img height="300px" width="300px" src="{{site.baseurl}}/assets/chatmodels.jpeg">
</center>

<br/>

<p style="text-align:justify">A lot has been said about models and fine-tuning them online. However, not much has been said about curating fine-tuning datasets. It's one of the most underrated yet crucial topics. As the old adage goes for machine learning, "Garbage in, Garbage out". The quality of data is paramount to any Machine learning/deep learning project. I have noticed people spending a lot of time trying larger and larger models without questioning their data. Even before large language models (LLMs) became popular, a wide variety of tools were being developed around the data-centric ecosystem. Even AI veterans like Andrew Ng <a href="https://mitsloan.mit.edu/ideas-made-to-matter/why-its-time-data-centric-artificial-intelligence">emphasized the importance</a> of data-centric machine learning. In this article, I intend to give you an overview as to how you can think about finetuning your LLM in a data centric manner. </p>
<p style="text-align:justify">The concept of data-centric finetuning for Language Models (LLMs) is based on having a complex enough model that can learn the required task. However, rather than spending too much effort trying to find the best model, the focus is on investing most of your time in obtaining the right data that allows you to achieve optimal results. The below diagram depicts the overall steps taken in data-centric finetuning. This article assumes you are familiar with the basics of deep learning. Note that the model is offcourse important, but this article emphasizes how given a good model you can get a lot done with enough good data without having to spend too much time on the model selection. </p>

<img height="300px" width="800px" src="{{site.baseurl}}/assets/overallcycle_datacentric.png">

<p style="text-align:justify">I will illustrate the process of gathering a dataset for a given task. Let me take an example of the process of doing finetuning a LLM for food reviews QA.</p>


<h2>Define your task clearly</h2>
<p style="text-align:justify">The first step is to clearly define your task as to what is the purpose of the model. What are the considerations? Speed? accuracy? memory? So the example I am taking is <b>Food Q&A</b>. That is we will have Question answering system in say a food reviews platform like doordash. Now why will a Q&A help? Mostly for User experience, Will it have to be fast? Yes. Will it have to be optimized for memory? not necessarily since it runs on the cloud, Will it have to be accurate? Yes, but for a limited set of queries. We don't want to acccomodate a query set as vast as say as ChatGPT or Grook.</p>

<center>
<img height="300px" width="300px" src="{{site.baseurl}}/assets/foodbot.png">
</center>

<br/>

<p style="text-align:justify">There are different QA variants based on the inputs and outputs:</p>
<ul>
<i>
  <li><b>Extractive QA</b>: The model extracts the answer from a context. The context here could be a provided text, a table, or even HTML! This is usually solved with BERT-like models.</li>
  <li><b>Open Generative QA</b>: The model generates free text directly based on the context. You can learn more about the Text Generation task on its page.</li>
  <li><b>Closed Generative QA</b>: In this case, no context is provided. The answer is completely generated by a model.</li>
</i>
</ul>
<p>Source of text: <a href="https://huggingface.co/tasks/question-answering">Huggingface</a></p>

<br/>

Extractive QA:
<blockquote>
  <p>Context:<br>
  The capital of France is Paris. The Eiffel Tower is a famous landmark located in the city.</p>
</blockquote>

<blockquote>
  <p>Question:<br>
  What is the capital of France?</p>
</blockquote>

<blockquote>
  <p>Extractive Answer:<br>
  The capital of France is Paris.</p>
</blockquote>

<br/>
Open Generative QA:



<blockquote>
  <p>Context:<br>
  A cat is curled up on a sunlit windowsill, peacefully dozing off.</p>
</blockquote>

<blockquote>
  <p>Question:<br>
  Describe the cat on the windowsill.</p>
</blockquote>

<blockquote>
  <p>Generative Answer:<br>
  A cat, peacefully dozing, rests on a sunlit windowsill.</p>
</blockquote>


<p style="text-align:justify">For our given problem Open Generative QA is appropriate where given a context and question, we rephrase the context in a manner that is friendly to a user. The task is not a simple extractive QA because we rephrase the context in a way that is user friendly to a given customer. We also don't used Closed Generative QA because we want the algorithm to use dynamic data that is updated everyday, so we don't have to train the model everyday based on latest reviews.
<h2>Understand your Data Universe</h2>
<p style="text-align:justify">In this step define all the kinds of data we would need. That is the possible kinds of questions you would want answered and how the context is to be structured. So in a typical Q&A LLM task, you would take context and question as input to produce an answer. So your training data would need tuples of question, context and answer. Your training data should closely resemble the kind of questions you would want to serve the users.

<p style="text-align:justify">Define all possible questions and use cases. For example, lets say the scope of questions you would want to serve are:

<ul>
  <li>Does <em>[specific restaurant]</em> offer <strong>[particular food]</strong> on its menu?</li>
  <li>Is <em>[specific restaurant]</em> known for serving <strong>[specific cuisine]</strong>?</li>
  <li>Can I bring my <strong>dog</strong> to <em>[specific restaurant]</em>?</li>
  <li>Does <em>[specific restaurant]</em> serve <strong>alcoholic beverages</strong>?</li>
  <li>Are there <strong>vegan options</strong> available at <em>[specific restaurant]</em>?</li>
  <li>What are the <strong>vegetarian options</strong> at <em>[specific restaurant]</em>?</li>
</ul>

<p style="text-align:justify">Now, don't just gather data for the intended behaviour, but also gather data for how you want your model to handle unintended behaviour.


<p style="text-align:justify">For example, a user could ask a question about restaurant A, but the retrieved context might be wrong. So your LLM should output something that it does not and not hallucinate or give incorrect information. For example, "Does restaurant 'Good Japan' provide car park?" and your Indexing algorithm might incorrectly retrieve a comment about good Japanese food. In a such a situation, your LLM must learn to answer it does not know. Offcourse designing your indexer better is a solution, but preparing your LLM for negative surprises is my point. Consider them as negative pairs.</p>

They are also very informative for your model to learn, as it provides more information to distinguish what a correct answer/wrong answer. Personally, this has provided much better performance when I was curating my dataset. When your algorithm learns the above as a negative example and learns that the right answer for a positive context.  </p>



<p style="text-align:justify">The negative examples are also very important pieces of information as it provides more clear signals for your algorithm to recognize a pattern.</p>

<p style="text-align:justify">Also add several variations of a single question such as without punctuation, with punctuation, etc. </p>


<p style="text-align:justify">Define them exhaustively. Remember your model can likely handle the kind of questions it has seen during training. The advantage of using a pretrained model trained on large datasets in unsupervised fashion is that you will have it learn patterns faster.
Also have your model understand synonyms and relations because its already seen such relations in unsupverised corpora. Having some idea of the specific dataset on which your pretrained model was trained on will give you some idea of what it already knows.
</p>



<h2>Curate a dataset</h2>
<p style="text-align:justify">Having established our dataset universe, proceed to collect data in the specified format. Follow standard machine learning practices by creating train/test/validation sets and pre-processing steps required.</p>


<h2>Find relevant pre-trained model</h2>

<p style="text-align:justify">Select a model pretrained model that closely matches your task.
For Question Answering based on context, we have two steps in the pipeline:</p>
<ol>
 <li><b>Indexing:</b> A model that picks the right context for given question</li>
 <li><b>QA Model:</b> The model that uses the context to answer a particular question</li>
</ol>
<p style="text-align:justify">For Indexing the simplest way to do it is to use an embedding model and perform similarity between question and embeddings of all available context using embedding models such as sentence transformers. Won't discuss much about indexing given that my focus is on developing QA model.</p>


<p style="text-align:justify">We can use a model as simple as GPT2 for the task. Lets try the pretrained model for the task. The default pretrained model does not work out of the box for the task.</p>
<center>
<img height="400px" width="1200px" src="{{site.baseurl}}/assets/codexample1.png">
</center>
<br/>

<p style="text-align:justify">Upon read the below description from the huggingface modelhub, you will notice that it's unlikely to be explicitly trained for the task we need.</p>

<br/>
<img height="200px" width="1200px" src="{{site.baseurl}}/assets/codexample2.png">
<br/>
<img height="200px" width="1200px" src="{{site.baseurl}}/assets/codexample3.png">

This is why we will need to finetune the model.
</p>

<h2>Choose your metrics</h2>
<p style="text-align:justify">Given that you have responses, how would you quantitatively define how good the response is? quantitatively evaluating models are important for any system since its a systematic manner of scalably evaluating them. It also allows us to choose loss functions that are more aligned to our intended objective. That is the LLM learns the way we want it. While, I won't dive deep into this topic but its a very important step because the whole process of developing LLM's in a data centric manner requires a good evaluation metric since collecting right data for LLM requires you to ensure what's incorrect/correct is being measured right. That is not just that you have a system that quantitatively is doing it right/wrong, but you are algorithm has a sense of whats right/wrong. A common metric for QA systems are F1-score which would be relevant for our system too.
RLHF
</p>
</p>




<h2>Finetune your model</h2>
<p style="text-align:justify">With the curated dataset and chosen metrics, fine-tune your LLM model using the standard training process, just like any other deep learning model.</p>
<h2>Evaluate and Gather more data</h2>
<p style="text-align:justify">Now evaluate your model using quantitative metrics established above. Look for examples on which your model did well and did not do well. Are there some kind of patterns you notice? The data did better on certain kinds of questions or worse on other kinds of questions? Now gather more data on such kind of questions. Although, we have tried our best to define our data universe as exhaustively as possible, there are always cases which we are likely to miss on. There will also be cases which were covered, but the algorithm still would not have learnt the right patterns. </p>
<p style="text-align:justify">If necessary, consider building a tool for dataset curation and inference. If you lack expertise in web development, you can utilize tools like ChatGPT or other Generative AI tools to create a straightforward interface. I have personally created a basic tool, as demonstrated below.</p>

<img height="400px" width="1200px" src="{{site.baseurl}}/assets/exampleannotate.png">

<p style="text-align:justify">It's a straightforward annotation webpage where you input a question and context to obtain an answer. For more complex projects, there are both good open-source and commercial solutions available. If the answer generated by the Language Model (LLM) on the webpage is unsatisfactory, you have the option to update it with the appropriate answer and include it in your dataset. These newly labeled instances will be utilized for fine-tuning the model in the next iteration. Repeat this process several more times until you achieve satisfactory performance. In this tool, I haven't included indexing because, in the application I was developing, the LLM and indexer were used together for choosing context. Therefore, I also ensured that the LLM was capable of functioning independently.</p>

Here are some articles that could give you insights into doing this process better: Andrej Karpathy and my article.</p>
